#!/usr/bin/env python3
"""
Generate .env file for Docker/self-hosted Letta deployments.

Usage:
    python generate_env.py --providers openai,anthropic
    python generate_env.py --providers openai,anthropic,ollama --output .env.production
    python generate_env.py --providers ollama --ollama-url http://localhost:11434
"""

import argparse
import sys
from pathlib import Path


PROVIDER_TEMPLATES = {
    "openai": """
# OpenAI Configuration
OPENAI_API_KEY=sk-your-openai-key-here
""",
    "anthropic": """
# Anthropic Configuration
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
""",
    "azure": """
# Azure OpenAI Configuration
AZURE_API_KEY=your-azure-key-here
AZURE_BASE_URL=https://your-resource.openai.azure.com
AZURE_API_VERSION=2024-09-01-preview
""",
    "google_ai": """
# Google AI Configuration
GOOGLE_AI_API_KEY=your-google-ai-key-here
""",
    "groq": """
# Groq Configuration
GROQ_API_KEY=your-groq-key-here
""",
    "ollama": """
# Ollama Configuration (local models)
OLLAMA_BASE_URL={ollama_url}
""",
    "deepseek": """
# DeepSeek Configuration
DEEPSEEK_API_KEY=your-deepseek-key-here
""",
    "together": """
# Together AI Configuration
TOGETHER_API_KEY=your-together-key-here
""",
    "mistral": """
# Mistral AI Configuration
MISTRAL_API_KEY=your-mistral-key-here
""",
    "xai": """
# xAI Configuration
XAI_API_KEY=your-xai-key-here
""",
}


def generate_env(providers: list[str], output: str = ".env", ollama_url: str = "http://host.docker.internal:11434"):
    """
    Generate .env file for Docker deployments.

    Args:
        providers: List of provider names
        output: Output file path
        ollama_url: Ollama endpoint URL (for Docker on macOS/Windows)
    """
    output_path = Path(output)

    # Check if file exists
    if output_path.exists():
        print(f"⚠ Warning: {output} already exists", file=sys.stderr)
        response = input("Overwrite? (y/n): ")
        if response.lower() != "y":
            print("Cancelled.")
            sys.exit(0)

    # Build content
    content = """# Letta Environment Configuration
# Generated by generate_env.py
#
# IMPORTANT: Replace placeholder values with your actual API keys
# DO NOT commit this file to version control

# Letta Server Configuration
LETTA_PG_URI=postgresql://letta:letta@localhost/letta
# LETTA_PG_URI=sqlite:///~/.letta/letta.db  # Alternative: SQLite

# Security (enable for production)
# SECURE=true
# LETTA_SERVER_PASSWORD=your-secure-password-here
"""

    # Add provider configurations
    for provider in providers:
        provider = provider.strip().lower()
        if provider in PROVIDER_TEMPLATES:
            template = PROVIDER_TEMPLATES[provider]
            if provider == "ollama":
                template = template.format(ollama_url=ollama_url)
            content += template
        else:
            print(f"⚠ Warning: Unknown provider '{provider}' - skipping", file=sys.stderr)

    # Add Docker networking note if Ollama is included
    if "ollama" in providers:
        content += """
# Docker Networking Notes:
# - macOS/Windows: Use host.docker.internal:11434
# - Linux: Use 172.17.0.1:11434 (or host network mode)
# - Make sure Ollama is configured with OLLAMA_HOST=0.0.0.0
"""

    # Write file
    try:
        output_path.write_text(content)
        print(f"✓ Generated {output}")
        print(f"\nNext steps:")
        print(f"1. Edit {output} and replace placeholder API keys")
        print(f"2. Run Docker with: docker run -p 8283:8283 --env-file {output} letta/letta:latest")
        print(f"\n⚠ IMPORTANT: Do NOT commit {output} to version control")

    except IOError as e:
        print(f"✗ Failed to write {output}: {e}", file=sys.stderr)
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(description="Generate .env file for Letta Docker deployments")

    parser.add_argument(
        "--providers",
        required=True,
        help="Comma-separated list of providers (e.g., openai,anthropic,ollama)",
    )
    parser.add_argument(
        "--output",
        default=".env",
        help="Output file path (default: .env)",
    )
    parser.add_argument(
        "--ollama-url",
        default="http://host.docker.internal:11434",
        help="Ollama endpoint URL (default: http://host.docker.internal:11434 for Docker on macOS/Windows)",
    )

    args = parser.parse_args()

    providers = [p.strip() for p in args.providers.split(",")]
    generate_env(providers=providers, output=args.output, ollama_url=args.ollama_url)


if __name__ == "__main__":
    main()
